{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Victim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "from torchvision import datasets, transforms, models\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some hyperparameters for the neural network \n",
    "\n",
    "batch_size = 100\n",
    "num_workers = 4\n",
    "learning_rate = 1e-3\n",
    "rand_split_val = [55000, 5000] #only relevant for MNIST\n",
    "gpus = 1\n",
    "max_epochs = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningClassifier(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    \"\"\"Defines a three layer fully connected neural network\"\"\"\n",
    "    super(LightningClassifier, self).__init__()\n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Establishes the neural network's forward pass\n",
    "    \n",
    "    Parameters: \n",
    "        x (Torch tensor): MNIST input image\n",
    "\n",
    "    Returns:\n",
    "        output probability vector for MNIST classes\n",
    "    \"\"\"\n",
    "    batch_size, channels, width, height = x.size()\n",
    "\n",
    "    # Input Layer: (batch_size, 1, 28, 28) -> (batch_size, 1*28*28)\n",
    "    x = x.view(batch_size, -1)\n",
    "\n",
    "    # Layer 1: (batch_size, 1*28*28) -> (batch_size, 128)\n",
    "    x = self.layer_1(x)\n",
    "    x = torch.relu(x)\n",
    "\n",
    "    # Layer 2: (batch_size, 128) -> (batch_size, 256)\n",
    "    x = self.layer_2(x)\n",
    "    x = torch.relu(x)\n",
    "\n",
    "    # Layer 3: (batch_size, 256) -> (batch_size, 10)\n",
    "    x = self.layer_3(x)\n",
    "    x = torch.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "  def cross_entropy_loss(self, logits, labels):\n",
    "    \"\"\"Calculates loss- the difference between model predictions and true labels\n",
    "    \n",
    "    Parameters:\n",
    "        logits (Torch tensor): model output predictions\n",
    "        labels (Torch tensor): true values for predictions \n",
    "        \n",
    "    Returns: \n",
    "        Cross entropy loss\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "    \"\"\"Pushes training data batch through model and calculates loss in loop\n",
    "\n",
    "    Parameters: \n",
    "        train_batch (Torch tensor): batch of training data from training dataloader \n",
    "        batch_idx (int): index of batch in contention\n",
    "\n",
    "    Returns: \n",
    "        Formatted string with cross entropy loss and training logs\n",
    "    \"\"\"\n",
    "    x, y = train_batch\n",
    "    logits = self.forward(x)\n",
    "    loss = self.cross_entropy_loss(logits, y)\n",
    "    logs = {'train_loss': loss}\n",
    "    return {'loss': loss, 'log': logs}\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "    \"\"\"Pushes validation data batch through model and calculates loss in loop\n",
    "\n",
    "    Parameters: \n",
    "        val_batch (Tensor): batch of validation data from validation dataloader \n",
    "        batch_idx (int): index of batch in contention\n",
    "\n",
    "    Returns: \n",
    "        Formatted string with resultant cross entropy loss \n",
    "    \"\"\"\n",
    "    x, y = val_batch\n",
    "    logits = self.forward(x)\n",
    "    loss = self.cross_entropy_loss(logits, y)\n",
    "    targets_hat = torch.argmax(logits, dim=1)\n",
    "    n_correct_pred = torch.sum(y == targets_hat).item()\n",
    "    return {'val_loss': loss, 'n_correct_pred': n_correct_pred, \"n_pred\": len(x)}\n",
    "\n",
    "  def validation_epoch_end(self, outputs):\n",
    "    \"\"\"Returns validation step results at the end of the epoch\n",
    "\n",
    "    Parameters: \n",
    "        outputs (array): result of validation step for each batch \n",
    "\n",
    "    Returns: \n",
    "        Formatted string with resultant metrics\n",
    "    \"\"\"\n",
    "    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "    tensorboard_logs = {'val_loss': avg_loss}\n",
    "    return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"\"\"Sets up the optimization scheme\"\"\"\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    return optimizer\n",
    "    \n",
    "  def test_step(self, batch, batch_idx):\n",
    "    \"\"\"Pushes test data into the model and returns relevant metrics\n",
    "    \n",
    "    Parameters:\n",
    "        batch (Torch tensor): batch of test data from test dataloader \n",
    "        batch_idx (int): index of batch in contention\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with relevant metrics\n",
    "    \"\"\"\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    targets_hat = torch.argmax(y_hat, dim=1)\n",
    "    n_correct_pred = torch.sum(y == targets_hat).item()\n",
    "    return {'test_loss': F.cross_entropy(y_hat, y), \"n_correct_pred\": n_correct_pred, \"n_pred\": len(x)}\n",
    "\n",
    "  def test_epoch_end(self, outputs):\n",
    "    \"\"\"Returns test step results at the end of the epoch\n",
    "\n",
    "    Parameters: \n",
    "        outputs (array): result of test step for each batch \n",
    "\n",
    "    Returns: \n",
    "        Formatted string with resultant metrics\n",
    "    \"\"\"\n",
    "    avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "    tensorboard_logs = {'test_loss': avg_loss}\n",
    "    return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(classifier, gpus, num_nodes, max_epochs, \n",
    "                   train_dataloader, val_dataloader, test_dataloader):\n",
    "    \"\"\"Instantiates, trains, and evaluates the given model on the test set\"\"\"\n",
    "    model = classifier()\n",
    "    trainer = pl.Trainer(gpus=gpus, num_nodes=num_nodes, max_epochs=max_epochs)\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    trainer.test(model, test_dataloaders=test_dataloader)\n",
    "    return model\n",
    "\n",
    "def convert_to_inference(model):\n",
    "    \"\"\"Allows a model to be used in an inference setting\"\"\"\n",
    "    model.freeze()\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_image(dataset, idx, cmap='gray'):\n",
    "    \"\"\"Shows a single datapoint from the test set as an image\n",
    "    \n",
    "    Parameters: \n",
    "        dataset (Torch dataset): test dataset to obtain image from \n",
    "        idx (int): index describing position of image \n",
    "        cmap (String): optional; defines color map for image \n",
    "        \n",
    "    Returns: \n",
    "        data sampled and displayed\n",
    "    \"\"\"\n",
    "    x, y = dataset[idx]\n",
    "    plt.imshow(x.numpy()[0], cmap=cmap)\n",
    "    return x\n",
    "\n",
    "def query_model(model, input_data, mnist):\n",
    "    \"\"\"Returns the predictions of a model\n",
    "    \n",
    "    Parameters: \n",
    "        model (pl.LightningModule): model to be queried \n",
    "        input_data (Torch tensor): the x for the model \n",
    "        mnist (Boolean): states if the model is MNIST \n",
    "        \n",
    "    Returns:\n",
    "        prediction (Torch tensor): predicton probabilities \n",
    "        np_prediction (Numpy array): predicton probabilities\n",
    "        target (int): predicted label  \n",
    "    \"\"\"\n",
    "    input_data = input_data.cuda()\n",
    "    if mnist == True:\n",
    "        print(\"Before\")\n",
    "        print(input_data.shape)\n",
    "        input_data = input_data.resize_(input_size)\n",
    "        input_data = input_data.resize_((1, 28, 28, 1))\n",
    "        #input_data = input_data.reshape((1, 28, 28, 1))\n",
    "        print(input_data.shape)\n",
    "        print(\"After\")\n",
    "    prediction = model(input_data)\n",
    "    np_prediction = prediction.cpu().numpy()\n",
    "    target = int(np.argmax(np_prediction))\n",
    "    return prediction, np_prediction, target\n",
    "\n",
    "def get_query_target(model, image, mnist):\n",
    "    \"\"\"Returns only the predicted label\"\"\"\n",
    "    prediction, np_prediction, target = query_model(model, image, mnist)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe transformations applied to MNIST data \n",
    "transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "#Download and split MNIST data\n",
    "mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
    "mnist_train, mnist_val = random_split(mnist_train, rand_split_val) \n",
    "\n",
    "#Create PyTorch DataLoaders\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=batch_size, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(mnist_val, batch_size=batch_size, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33 K  \n",
      "2 | layer_3 | Linear | 2 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7b7599cc1b4919912566d7809bd505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffdd91b0a044efba73da782943bf57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacca051429f4b848f3ecc7baa106411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhacker/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Did not find hyperparameters at model hparams. Saving checkpoint without hyperparameters.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f4a14d5b3346fd947ab526f0962438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eb3385cf844e808d8f86ba7eb34c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f65d380801445aad492864e05a5070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db97e44a7c4c4a3183888f33bb65c7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43130a6992f4191839d03800a5a072b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7a122e0a324f2a8f5c031abd805f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfc5e8ca4e742b3b25623b25d6dbb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc07277b2a04d7291bcffd9a5ae3b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_loss': tensor(0.0974, device='cuda:0'),\n",
      " 'test_loss': tensor(0.0974, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop; evaluate model \n",
    "mnist_model = train_and_test(LightningClassifier, gpus, 1, max_epochs, \n",
    "                             train_dataloader, val_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_model = convert_to_inference(mnist_model)\n",
    "sample_image = show_test_image(mnist_test, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n"
     ]
    }
   ],
   "source": [
    "pred_x, np_pred_x, label_x = query_model(mnist_model, sample_image, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(get_query_target(mnist_model, sample_image, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Substitute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe transformations for EMNIST \n",
    "#The conversion to RGB is included for the feature extractor \n",
    "transform = transforms.Compose([transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "#Download EMNIST dataset\n",
    "emnist_train = EMNIST(os.getcwd(), split=\"digits\", train=True, download=True, transform=transform)\n",
    "emnist_test = EMNIST(os.getcwd(), split=\"digits\",  train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = emnist_train[0]\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1213.37it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1109.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "torch.Size([3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before reshape\n",
      "torch.Size([3, 28, 28])\n",
      "After reshape\n",
      "torch.Size([1, 3, 28, 28])\n",
      "Before\n",
      "torch.Size([3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Training Set Created: torch.Size([5, 3, 28, 28])\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Before\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "After\n",
      "Testing Set Created: torch.Size([5, 3, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate synthetic training and testing data\n",
    "#Can be made more efficient with vectorization\n",
    "\n",
    "#Generate first datapoint \n",
    "emnist_x_train, y0 = emnist_train[0]\n",
    "emnist_y_train = torch.tensor([get_query_target(mnist_model, emnist_x_train, True)])\n",
    "print(\"Before reshape\")\n",
    "print(emnist_x_train.shape)\n",
    "emnist_x_train= emnist_x_train.reshape((1, 3, 28, 28))\n",
    "print(\"After reshape\")\n",
    "print(emnist_x_train.shape)\n",
    "\n",
    "emnist_x_test, y0 = emnist_test[0]\n",
    "emnist_y_test = torch.tensor([get_query_target(mnist_model, emnist_x_test, True)])\n",
    "emnist_x_test= emnist_x_test.reshape((1, 3, 28, 28))\n",
    "\n",
    "#Generate training data\n",
    "k = 5\n",
    "for i in tqdm(range(1, k)):\n",
    "    x, y = emnist_train[i]\n",
    "    x = x.reshape((1, 3, 28, 28))\n",
    "    emnist_x_train = torch.cat((emnist_x_train, x))\n",
    "    y_hat = torch.tensor([get_query_target(mnist_model, x, True)])\n",
    "    emnist_y_train = torch.cat((emnist_y_train, y_hat))\n",
    "    \n",
    "print(\"Training Set Created: \" + str(emnist_x_train.shape))\n",
    "\n",
    "#Generate testing data\n",
    "j = 5\n",
    "for i in tqdm(range(1, j)):\n",
    "    xt, yt = emnist_test[i]\n",
    "    xt = xt.reshape((1, 3, 28, 28))\n",
    "    emnist_x_test = torch.cat((emnist_x_test, xt))\n",
    "    y_hat_t = torch.tensor([get_query_target(mnist_model, xt, True)])\n",
    "    emnist_y_test = torch.cat((emnist_y_test, y_hat_t))\n",
    "        \n",
    "print(\"Testing Set Created: \"+ str(emnist_x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create separate validation and training data\n",
    "emnist_x_train, emnist_x_valid, emnist_y_train, emnist_y_valid = train_test_split(\n",
    "    emnist_x_train, \n",
    "    emnist_y_train, \n",
    "    test_size= 0.4, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataset class and loaders\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, targets, transform=None):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image.numpy())\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish datasets and dataloaders\n",
    "emnist_train = CustomDataset(emnist_x_train, emnist_y_train)\n",
    "emnist_valid = CustomDataset(emnist_x_valid, emnist_y_valid)\n",
    "emnist_test = CustomDataset(emnist_x_test, emnist_y_test)\n",
    "\n",
    "emnist_train_dataloader = DataLoader(emnist_train, batch_size=batch_size, \n",
    "                                     num_workers=num_workers)\n",
    "emnist_val_dataloader = DataLoader(emnist_valid, batch_size=batch_size, num_workers=num_workers)\n",
    "emnist_test_dataloader = DataLoader(emnist_test, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetTransferLearning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        \"\"\"\"Create a classifier with a pretrained MobileNet backbone\"\"\"\"\n",
    "        super(ImagenetTransferLearning, self).__init__()\n",
    "        num_target_classes = 10\n",
    "        self.feature_extractor = models.mobilenet_v2(pretrained=True)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        #Establish classifier\n",
    "        self.layer_1 = torch.nn.Linear(1000, 128)\n",
    "        self.layer_2 = torch.nn.Linear(128, 256)\n",
    "        self.layer_3 = torch.nn.Linear(256, num_target_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Establishes the neural network's forward pass\n",
    "        \n",
    "        Parameters: \n",
    "            x (Torch tensor): input image\n",
    "            \n",
    "        Returns:\n",
    "            utput probability vector for classes\n",
    "        \"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        batch_size, hidden = x.size()\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def nll_loss(self, logits, labels):\n",
    "        \"\"\"Calculates loss\n",
    "        \n",
    "        Parameters:\n",
    "            logits (Torch tensor): model output predictions\n",
    "            labels (Torch tensor): true values for predictions\n",
    "            \n",
    "        Returns: \n",
    "            Loss\n",
    "        \"\"\"\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \"\"\"Pushes training data batch through model and calculates loss in loop\n",
    "\n",
    "        Parameters: \n",
    "            train_batch (Torch tensor): batch of training data from training dataloader \n",
    "            batch_idx (int): index of batch in contention\n",
    "\n",
    "        Returns: \n",
    "            Formatted string with cross entropy loss and training logs\n",
    "        \"\"\"\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.nll_loss(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        \"\"\"Pushes validation data batch through model and calculates loss in loop\n",
    "        \n",
    "        Parameters: \n",
    "            val_batch (Tensor): batch of validation data from validation dataloader \n",
    "            batch_idx (int): index of batch in contention\n",
    "\n",
    "        Returns: \n",
    "            Formatted string with resultant cross entropy loss \n",
    "        \"\"\"\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.nll_loss(logits, y)\n",
    "        targets_hat = torch.argmax(logits, dim=1)\n",
    "        n_correct_pred = torch.sum(y == targets_hat).item()\n",
    "        return {'val_loss': loss, 'n_correct_pred': n_correct_pred, \"n_pred\": len(x)}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"Returns validation step results at the end of the epoch\n",
    "\n",
    "        Parameters: \n",
    "            outputs (array): result of validation step for each batch \n",
    "\n",
    "        Returns: \n",
    "            Formatted string with resultant metrics\n",
    "        \"\"\"\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Sets up the optimization scheme\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Pushes test data into the model and returns relevant metrics\n",
    "\n",
    "        Parameters:\n",
    "            batch (Torch tensor): batch of test data from test dataloader \n",
    "            batch_idx (int): index of batch in contention\n",
    "\n",
    "        Returns:\n",
    "            Formatted string with relevant metrics\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        targets_hat = torch.argmax(y_hat, dim=1)\n",
    "        n_correct_pred = torch.sum(y == targets_hat).item()\n",
    "        return {'test_loss': F.nll_loss(y_hat, y), \"n_correct_pred\": n_correct_pred, \"n_pred\": len(x)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        \"\"\"Returns test step results at the end of the epoch\n",
    "\n",
    "        Parameters: \n",
    "            outputs (array): result of test step for each batch \n",
    "\n",
    "        Returns: \n",
    "            Formatted string with resultant metrics\n",
    "        \"\"\"\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "    | Name                                   | Type             | Params\n",
      "------------------------------------------------------------------------\n",
      "0   | feature_extractor                      | MobileNetV2      | 3 M   \n",
      "1   | feature_extractor.features             | Sequential       | 2 M   \n",
      "2   | feature_extractor.features.0           | ConvBNReLU       | 928   \n",
      "3   | feature_extractor.features.0.0         | Conv2d           | 864   \n",
      "4   | feature_extractor.features.0.1         | BatchNorm2d      | 64    \n",
      "5   | feature_extractor.features.0.2         | ReLU6            | 0     \n",
      "6   | feature_extractor.features.1           | InvertedResidual | 896   \n",
      "7   | feature_extractor.features.1.conv      | Sequential       | 896   \n",
      "8   | feature_extractor.features.1.conv.0    | ConvBNReLU       | 352   \n",
      "9   | feature_extractor.features.1.conv.0.0  | Conv2d           | 288   \n",
      "10  | feature_extractor.features.1.conv.0.1  | BatchNorm2d      | 64    \n",
      "11  | feature_extractor.features.1.conv.0.2  | ReLU6            | 0     \n",
      "12  | feature_extractor.features.1.conv.1    | Conv2d           | 512   \n",
      "13  | feature_extractor.features.1.conv.2    | BatchNorm2d      | 32    \n",
      "14  | feature_extractor.features.2           | InvertedResidual | 5 K   \n",
      "15  | feature_extractor.features.2.conv      | Sequential       | 5 K   \n",
      "16  | feature_extractor.features.2.conv.0    | ConvBNReLU       | 1 K   \n",
      "17  | feature_extractor.features.2.conv.0.0  | Conv2d           | 1 K   \n",
      "18  | feature_extractor.features.2.conv.0.1  | BatchNorm2d      | 192   \n",
      "19  | feature_extractor.features.2.conv.0.2  | ReLU6            | 0     \n",
      "20  | feature_extractor.features.2.conv.1    | ConvBNReLU       | 1 K   \n",
      "21  | feature_extractor.features.2.conv.1.0  | Conv2d           | 864   \n",
      "22  | feature_extractor.features.2.conv.1.1  | BatchNorm2d      | 192   \n",
      "23  | feature_extractor.features.2.conv.1.2  | ReLU6            | 0     \n",
      "24  | feature_extractor.features.2.conv.2    | Conv2d           | 2 K   \n",
      "25  | feature_extractor.features.2.conv.3    | BatchNorm2d      | 48    \n",
      "26  | feature_extractor.features.3           | InvertedResidual | 8 K   \n",
      "27  | feature_extractor.features.3.conv      | Sequential       | 8 K   \n",
      "28  | feature_extractor.features.3.conv.0    | ConvBNReLU       | 3 K   \n",
      "29  | feature_extractor.features.3.conv.0.0  | Conv2d           | 3 K   \n",
      "30  | feature_extractor.features.3.conv.0.1  | BatchNorm2d      | 288   \n",
      "31  | feature_extractor.features.3.conv.0.2  | ReLU6            | 0     \n",
      "32  | feature_extractor.features.3.conv.1    | ConvBNReLU       | 1 K   \n",
      "33  | feature_extractor.features.3.conv.1.0  | Conv2d           | 1 K   \n",
      "34  | feature_extractor.features.3.conv.1.1  | BatchNorm2d      | 288   \n",
      "35  | feature_extractor.features.3.conv.1.2  | ReLU6            | 0     \n",
      "36  | feature_extractor.features.3.conv.2    | Conv2d           | 3 K   \n",
      "37  | feature_extractor.features.3.conv.3    | BatchNorm2d      | 48    \n",
      "38  | feature_extractor.features.4           | InvertedResidual | 10 K  \n",
      "39  | feature_extractor.features.4.conv      | Sequential       | 10 K  \n",
      "40  | feature_extractor.features.4.conv.0    | ConvBNReLU       | 3 K   \n",
      "41  | feature_extractor.features.4.conv.0.0  | Conv2d           | 3 K   \n",
      "42  | feature_extractor.features.4.conv.0.1  | BatchNorm2d      | 288   \n",
      "43  | feature_extractor.features.4.conv.0.2  | ReLU6            | 0     \n",
      "44  | feature_extractor.features.4.conv.1    | ConvBNReLU       | 1 K   \n",
      "45  | feature_extractor.features.4.conv.1.0  | Conv2d           | 1 K   \n",
      "46  | feature_extractor.features.4.conv.1.1  | BatchNorm2d      | 288   \n",
      "47  | feature_extractor.features.4.conv.1.2  | ReLU6            | 0     \n",
      "48  | feature_extractor.features.4.conv.2    | Conv2d           | 4 K   \n",
      "49  | feature_extractor.features.4.conv.3    | BatchNorm2d      | 64    \n",
      "50  | feature_extractor.features.5           | InvertedResidual | 14 K  \n",
      "51  | feature_extractor.features.5.conv      | Sequential       | 14 K  \n",
      "52  | feature_extractor.features.5.conv.0    | ConvBNReLU       | 6 K   \n",
      "53  | feature_extractor.features.5.conv.0.0  | Conv2d           | 6 K   \n",
      "54  | feature_extractor.features.5.conv.0.1  | BatchNorm2d      | 384   \n",
      "55  | feature_extractor.features.5.conv.0.2  | ReLU6            | 0     \n",
      "56  | feature_extractor.features.5.conv.1    | ConvBNReLU       | 2 K   \n",
      "57  | feature_extractor.features.5.conv.1.0  | Conv2d           | 1 K   \n",
      "58  | feature_extractor.features.5.conv.1.1  | BatchNorm2d      | 384   \n",
      "59  | feature_extractor.features.5.conv.1.2  | ReLU6            | 0     \n",
      "60  | feature_extractor.features.5.conv.2    | Conv2d           | 6 K   \n",
      "61  | feature_extractor.features.5.conv.3    | BatchNorm2d      | 64    \n",
      "62  | feature_extractor.features.6           | InvertedResidual | 14 K  \n",
      "63  | feature_extractor.features.6.conv      | Sequential       | 14 K  \n",
      "64  | feature_extractor.features.6.conv.0    | ConvBNReLU       | 6 K   \n",
      "65  | feature_extractor.features.6.conv.0.0  | Conv2d           | 6 K   \n",
      "66  | feature_extractor.features.6.conv.0.1  | BatchNorm2d      | 384   \n",
      "67  | feature_extractor.features.6.conv.0.2  | ReLU6            | 0     \n",
      "68  | feature_extractor.features.6.conv.1    | ConvBNReLU       | 2 K   \n",
      "69  | feature_extractor.features.6.conv.1.0  | Conv2d           | 1 K   \n",
      "70  | feature_extractor.features.6.conv.1.1  | BatchNorm2d      | 384   \n",
      "71  | feature_extractor.features.6.conv.1.2  | ReLU6            | 0     \n",
      "72  | feature_extractor.features.6.conv.2    | Conv2d           | 6 K   \n",
      "73  | feature_extractor.features.6.conv.3    | BatchNorm2d      | 64    \n",
      "74  | feature_extractor.features.7           | InvertedResidual | 21 K  \n",
      "75  | feature_extractor.features.7.conv      | Sequential       | 21 K  \n",
      "76  | feature_extractor.features.7.conv.0    | ConvBNReLU       | 6 K   \n",
      "77  | feature_extractor.features.7.conv.0.0  | Conv2d           | 6 K   \n",
      "78  | feature_extractor.features.7.conv.0.1  | BatchNorm2d      | 384   \n",
      "79  | feature_extractor.features.7.conv.0.2  | ReLU6            | 0     \n",
      "80  | feature_extractor.features.7.conv.1    | ConvBNReLU       | 2 K   \n",
      "81  | feature_extractor.features.7.conv.1.0  | Conv2d           | 1 K   \n",
      "82  | feature_extractor.features.7.conv.1.1  | BatchNorm2d      | 384   \n",
      "83  | feature_extractor.features.7.conv.1.2  | ReLU6            | 0     \n",
      "84  | feature_extractor.features.7.conv.2    | Conv2d           | 12 K  \n",
      "85  | feature_extractor.features.7.conv.3    | BatchNorm2d      | 128   \n",
      "86  | feature_extractor.features.8           | InvertedResidual | 54 K  \n",
      "87  | feature_extractor.features.8.conv      | Sequential       | 54 K  \n",
      "88  | feature_extractor.features.8.conv.0    | ConvBNReLU       | 25 K  \n",
      "89  | feature_extractor.features.8.conv.0.0  | Conv2d           | 24 K  \n",
      "90  | feature_extractor.features.8.conv.0.1  | BatchNorm2d      | 768   \n",
      "91  | feature_extractor.features.8.conv.0.2  | ReLU6            | 0     \n",
      "92  | feature_extractor.features.8.conv.1    | ConvBNReLU       | 4 K   \n",
      "93  | feature_extractor.features.8.conv.1.0  | Conv2d           | 3 K   \n",
      "94  | feature_extractor.features.8.conv.1.1  | BatchNorm2d      | 768   \n",
      "95  | feature_extractor.features.8.conv.1.2  | ReLU6            | 0     \n",
      "96  | feature_extractor.features.8.conv.2    | Conv2d           | 24 K  \n",
      "97  | feature_extractor.features.8.conv.3    | BatchNorm2d      | 128   \n",
      "98  | feature_extractor.features.9           | InvertedResidual | 54 K  \n",
      "99  | feature_extractor.features.9.conv      | Sequential       | 54 K  \n",
      "100 | feature_extractor.features.9.conv.0    | ConvBNReLU       | 25 K  \n",
      "101 | feature_extractor.features.9.conv.0.0  | Conv2d           | 24 K  \n",
      "102 | feature_extractor.features.9.conv.0.1  | BatchNorm2d      | 768   \n",
      "103 | feature_extractor.features.9.conv.0.2  | ReLU6            | 0     \n",
      "104 | feature_extractor.features.9.conv.1    | ConvBNReLU       | 4 K   \n",
      "105 | feature_extractor.features.9.conv.1.0  | Conv2d           | 3 K   \n",
      "106 | feature_extractor.features.9.conv.1.1  | BatchNorm2d      | 768   \n",
      "107 | feature_extractor.features.9.conv.1.2  | ReLU6            | 0     \n",
      "108 | feature_extractor.features.9.conv.2    | Conv2d           | 24 K  \n",
      "109 | feature_extractor.features.9.conv.3    | BatchNorm2d      | 128   \n",
      "110 | feature_extractor.features.10          | InvertedResidual | 54 K  \n",
      "111 | feature_extractor.features.10.conv     | Sequential       | 54 K  \n",
      "112 | feature_extractor.features.10.conv.0   | ConvBNReLU       | 25 K  \n",
      "113 | feature_extractor.features.10.conv.0.0 | Conv2d           | 24 K  \n",
      "114 | feature_extractor.features.10.conv.0.1 | BatchNorm2d      | 768   \n",
      "115 | feature_extractor.features.10.conv.0.2 | ReLU6            | 0     \n",
      "116 | feature_extractor.features.10.conv.1   | ConvBNReLU       | 4 K   \n",
      "117 | feature_extractor.features.10.conv.1.0 | Conv2d           | 3 K   \n",
      "118 | feature_extractor.features.10.conv.1.1 | BatchNorm2d      | 768   \n",
      "119 | feature_extractor.features.10.conv.1.2 | ReLU6            | 0     \n",
      "120 | feature_extractor.features.10.conv.2   | Conv2d           | 24 K  \n",
      "121 | feature_extractor.features.10.conv.3   | BatchNorm2d      | 128   \n",
      "122 | feature_extractor.features.11          | InvertedResidual | 66 K  \n",
      "123 | feature_extractor.features.11.conv     | Sequential       | 66 K  \n",
      "124 | feature_extractor.features.11.conv.0   | ConvBNReLU       | 25 K  \n",
      "125 | feature_extractor.features.11.conv.0.0 | Conv2d           | 24 K  \n",
      "126 | feature_extractor.features.11.conv.0.1 | BatchNorm2d      | 768   \n",
      "127 | feature_extractor.features.11.conv.0.2 | ReLU6            | 0     \n",
      "128 | feature_extractor.features.11.conv.1   | ConvBNReLU       | 4 K   \n",
      "129 | feature_extractor.features.11.conv.1.0 | Conv2d           | 3 K   \n",
      "130 | feature_extractor.features.11.conv.1.1 | BatchNorm2d      | 768   \n",
      "131 | feature_extractor.features.11.conv.1.2 | ReLU6            | 0     \n",
      "132 | feature_extractor.features.11.conv.2   | Conv2d           | 36 K  \n",
      "133 | feature_extractor.features.11.conv.3   | BatchNorm2d      | 192   \n",
      "134 | feature_extractor.features.12          | InvertedResidual | 118 K \n",
      "135 | feature_extractor.features.12.conv     | Sequential       | 118 K \n",
      "136 | feature_extractor.features.12.conv.0   | ConvBNReLU       | 56 K  \n",
      "137 | feature_extractor.features.12.conv.0.0 | Conv2d           | 55 K  \n",
      "138 | feature_extractor.features.12.conv.0.1 | BatchNorm2d      | 1 K   \n",
      "139 | feature_extractor.features.12.conv.0.2 | ReLU6            | 0     \n",
      "140 | feature_extractor.features.12.conv.1   | ConvBNReLU       | 6 K   \n",
      "141 | feature_extractor.features.12.conv.1.0 | Conv2d           | 5 K   \n",
      "142 | feature_extractor.features.12.conv.1.1 | BatchNorm2d      | 1 K   \n",
      "143 | feature_extractor.features.12.conv.1.2 | ReLU6            | 0     \n",
      "144 | feature_extractor.features.12.conv.2   | Conv2d           | 55 K  \n",
      "145 | feature_extractor.features.12.conv.3   | BatchNorm2d      | 192   \n",
      "146 | feature_extractor.features.13          | InvertedResidual | 118 K \n",
      "147 | feature_extractor.features.13.conv     | Sequential       | 118 K \n",
      "148 | feature_extractor.features.13.conv.0   | ConvBNReLU       | 56 K  \n",
      "149 | feature_extractor.features.13.conv.0.0 | Conv2d           | 55 K  \n",
      "150 | feature_extractor.features.13.conv.0.1 | BatchNorm2d      | 1 K   \n",
      "151 | feature_extractor.features.13.conv.0.2 | ReLU6            | 0     \n",
      "152 | feature_extractor.features.13.conv.1   | ConvBNReLU       | 6 K   \n",
      "153 | feature_extractor.features.13.conv.1.0 | Conv2d           | 5 K   \n",
      "154 | feature_extractor.features.13.conv.1.1 | BatchNorm2d      | 1 K   \n",
      "155 | feature_extractor.features.13.conv.1.2 | ReLU6            | 0     \n",
      "156 | feature_extractor.features.13.conv.2   | Conv2d           | 55 K  \n",
      "157 | feature_extractor.features.13.conv.3   | BatchNorm2d      | 192   \n",
      "158 | feature_extractor.features.14          | InvertedResidual | 155 K \n",
      "159 | feature_extractor.features.14.conv     | Sequential       | 155 K \n",
      "160 | feature_extractor.features.14.conv.0   | ConvBNReLU       | 56 K  \n",
      "161 | feature_extractor.features.14.conv.0.0 | Conv2d           | 55 K  \n",
      "162 | feature_extractor.features.14.conv.0.1 | BatchNorm2d      | 1 K   \n",
      "163 | feature_extractor.features.14.conv.0.2 | ReLU6            | 0     \n",
      "164 | feature_extractor.features.14.conv.1   | ConvBNReLU       | 6 K   \n",
      "165 | feature_extractor.features.14.conv.1.0 | Conv2d           | 5 K   \n",
      "166 | feature_extractor.features.14.conv.1.1 | BatchNorm2d      | 1 K   \n",
      "167 | feature_extractor.features.14.conv.1.2 | ReLU6            | 0     \n",
      "168 | feature_extractor.features.14.conv.2   | Conv2d           | 92 K  \n",
      "169 | feature_extractor.features.14.conv.3   | BatchNorm2d      | 320   \n",
      "170 | feature_extractor.features.15          | InvertedResidual | 320 K \n",
      "171 | feature_extractor.features.15.conv     | Sequential       | 320 K \n",
      "172 | feature_extractor.features.15.conv.0   | ConvBNReLU       | 155 K \n",
      "173 | feature_extractor.features.15.conv.0.0 | Conv2d           | 153 K \n",
      "174 | feature_extractor.features.15.conv.0.1 | BatchNorm2d      | 1 K   \n",
      "175 | feature_extractor.features.15.conv.0.2 | ReLU6            | 0     \n",
      "176 | feature_extractor.features.15.conv.1   | ConvBNReLU       | 10 K  \n",
      "177 | feature_extractor.features.15.conv.1.0 | Conv2d           | 8 K   \n",
      "178 | feature_extractor.features.15.conv.1.1 | BatchNorm2d      | 1 K   \n",
      "179 | feature_extractor.features.15.conv.1.2 | ReLU6            | 0     \n",
      "180 | feature_extractor.features.15.conv.2   | Conv2d           | 153 K \n",
      "181 | feature_extractor.features.15.conv.3   | BatchNorm2d      | 320   \n",
      "182 | feature_extractor.features.16          | InvertedResidual | 320 K \n",
      "183 | feature_extractor.features.16.conv     | Sequential       | 320 K \n",
      "184 | feature_extractor.features.16.conv.0   | ConvBNReLU       | 155 K \n",
      "185 | feature_extractor.features.16.conv.0.0 | Conv2d           | 153 K \n",
      "186 | feature_extractor.features.16.conv.0.1 | BatchNorm2d      | 1 K   \n",
      "187 | feature_extractor.features.16.conv.0.2 | ReLU6            | 0     \n",
      "188 | feature_extractor.features.16.conv.1   | ConvBNReLU       | 10 K  \n",
      "189 | feature_extractor.features.16.conv.1.0 | Conv2d           | 8 K   \n",
      "190 | feature_extractor.features.16.conv.1.1 | BatchNorm2d      | 1 K   \n",
      "191 | feature_extractor.features.16.conv.1.2 | ReLU6            | 0     \n",
      "192 | feature_extractor.features.16.conv.2   | Conv2d           | 153 K \n",
      "193 | feature_extractor.features.16.conv.3   | BatchNorm2d      | 320   \n",
      "194 | feature_extractor.features.17          | InvertedResidual | 473 K \n",
      "195 | feature_extractor.features.17.conv     | Sequential       | 473 K \n",
      "196 | feature_extractor.features.17.conv.0   | ConvBNReLU       | 155 K \n",
      "197 | feature_extractor.features.17.conv.0.0 | Conv2d           | 153 K \n",
      "198 | feature_extractor.features.17.conv.0.1 | BatchNorm2d      | 1 K   \n",
      "199 | feature_extractor.features.17.conv.0.2 | ReLU6            | 0     \n",
      "200 | feature_extractor.features.17.conv.1   | ConvBNReLU       | 10 K  \n",
      "201 | feature_extractor.features.17.conv.1.0 | Conv2d           | 8 K   \n",
      "202 | feature_extractor.features.17.conv.1.1 | BatchNorm2d      | 1 K   \n",
      "203 | feature_extractor.features.17.conv.1.2 | ReLU6            | 0     \n",
      "204 | feature_extractor.features.17.conv.2   | Conv2d           | 307 K \n",
      "205 | feature_extractor.features.17.conv.3   | BatchNorm2d      | 640   \n",
      "206 | feature_extractor.features.18          | ConvBNReLU       | 412 K \n",
      "207 | feature_extractor.features.18.0        | Conv2d           | 409 K \n",
      "208 | feature_extractor.features.18.1        | BatchNorm2d      | 2 K   \n",
      "209 | feature_extractor.features.18.2        | ReLU6            | 0     \n",
      "210 | feature_extractor.classifier           | Sequential       | 1 M   \n",
      "211 | feature_extractor.classifier.0         | Dropout          | 0     \n",
      "212 | feature_extractor.classifier.1         | Linear           | 1 M   \n",
      "213 | layer_1                                | Linear           | 128 K \n",
      "214 | layer_2                                | Linear           | 33 K  \n",
      "215 | layer_3                                | Linear           | 2 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c6873377ce4097b6c5fb3315702546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a19083f172490d96323a01c5fb2b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9628f7ad872e45138d786153d8f1dfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3798b78b6604d57963b1c8db415b959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9207671bece7451bbc4a6a90094bf876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458827fbe4e14f52b6c018abfa4e3ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c2b96cf50d40e185275284a23364e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a072206580ef49c5b1d96e6c599323f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c09376462e648cf838661ebf7c4959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e32e89764b7400dbce13e1d21855604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcbae1ea61a40a39de5175ce9b0355d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_loss': tensor(0.5969, device='cuda:0'),\n",
      " 'test_loss': tensor(0.5969, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train and test substitute model \n",
    "emnist_model = train_and_test(ImagenetTransferLearning, gpus, 1, \n",
    "                              max_epochs, emnist_train_dataloader, \n",
    "                              emnist_val_dataloader, emnist_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_model = convert_to_inference(emnist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 134.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The substitute model thinks this is a 7\n",
      "The original model thinks this is a 7\n",
      "The substitute model thinks this is a 9\n",
      "The original model thinks this is a 8\n",
      "The substitute model thinks this is a 2\n",
      "The original model thinks this is a 2\n",
      "The substitute model thinks this is a 4\n",
      "The original model thinks this is a 4\n",
      "The substitute model thinks this is a 0\n",
      "The original model thinks this is a 0\n",
      "The substitute model thinks this is a 0\n",
      "The original model thinks this is a 0\n",
      "The substitute model thinks this is a 5\n",
      "The original model thinks this is a 5\n",
      "The substitute model thinks this is a 0\n",
      "The original model thinks this is a 0\n",
      "The substitute model thinks this is a 0\n",
      "The original model thinks this is a 0\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Determine label agreement between victim and substitute model\n",
    "\n",
    "agreed = 0 \n",
    "len_data = 100 \n",
    "\n",
    "for i in tqdm(range(1, len_data)):\n",
    "\n",
    "    x, y = emnist_test[i] \n",
    "    x = x.reshape((1, 3, 28, 28))\n",
    "    \n",
    "    emnist_result = int(get_query_target(emnist_model, x, False))\n",
    "    mnist_result = int(get_query_target(mnist_model, x, True))\n",
    "    \n",
    "    ##Uncomment to print predictions per image: \n",
    "    \n",
    "    #print(\"The substitute model thinks this is a \" + str(emnist_result))\n",
    "    #print(\"The original model thinks this is a \" + str(mnist_result))\n",
    "    \n",
    "    if emnist_result == mnist_result: \n",
    "        agreed = agreed + 1  \n",
    "                       \n",
    "print(\"Out of \" + str(len_data) + \" data points, the models agreed upon \" + str(agreed) + \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
